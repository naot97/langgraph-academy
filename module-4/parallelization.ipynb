{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0e279f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/parallelization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239934-lesson-1-parallelization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4169bfb-769a-4db3-833e-c827f19024b2",
   "metadata": {},
   "source": [
    "# Parallel node execution\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 3, we went in-depth on `human-in-the loop`, showing 3 common use-cases:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will build on `human-in-the-loop` as well as the `memory` concepts discussed in module 2.\n",
    "\n",
    "We will dive into `multi-agent` workflows, and build up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent research assistant, we'll first discuss a few LangGraph controllability topics.\n",
    "\n",
    "We'll start with [parallelization](https://langchain-ai.github.io/langgraph/how-tos/branching/#how-to-create-branches-for-parallel-node-execution).\n",
    "\n",
    "## Fan out and fan in\n",
    "\n",
    "Let's build a simple linear graph that over-writes the state at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U  langgraph tavily-python wikipedia langchain_openai langchain_community langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bbec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"AZURE_OPENAI_API_KEY\")\n",
    "_set_env(\"AZURE_OPENAI_ENDPOINT\")\n",
    "_set_env(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd77093-1794-4bd7-8c57-58f59a74c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: str\n",
    "\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['state']}\")\n",
    "        return {\"state\": [self._value]}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"b\", \"c\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd027d3-ef9f-4d43-b190-e9f07d521e18",
   "metadata": {},
   "source": [
    "We over-write state, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf260088-90d5-45b2-93ab-42f241560840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding I'm A to []\n",
      "Adding I'm B to [\"I'm A\"]\n",
      "Adding I'm C to [\"I'm B\"]\n",
      "Adding I'm D to [\"I'm C\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': [\"I'm D\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec27d-dc43-4088-beb2-53ad090d2971",
   "metadata": {},
   "source": [
    "Now, let's run `b` and `c` in parallel. \n",
    "\n",
    "And then run `d`.\n",
    "\n",
    "We can do this easily with fan-out from `a` to `b` and `c`, and then fan-in to `d`.\n",
    "\n",
    "The the state updates are applied at the end of each step.\n",
    "\n",
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdeaaab-a8c3-470f-89ef-9cf0a2760667",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35238fde-0230-4ae8-9200-158a8835c4f1",
   "metadata": {},
   "source": [
    "**We see an error**! \n",
    "\n",
    "This is because both `b` and `c` are writing to the same state key / channel in the same step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9048b041-6849-4f09-9811-6b7a80f67859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding I'm A to []\n",
      "Adding I'm B to [\"I'm A\"]\n",
      "Adding I'm C to [\"I'm A\"]\n",
      "An error occurred: At key 'state': Can receive only one value per step. Use an Annotated key to handle multiple values.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE\n"
     ]
    }
   ],
   "source": [
    "from langgraph.errors import InvalidUpdateError\n",
    "try:\n",
    "    graph.invoke({\"state\": []})\n",
    "except InvalidUpdateError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc329d-59fa-4c26-adcf-9122a824955d",
   "metadata": {},
   "source": [
    "When using fan out, we need to be sure that we are using a reducer if steps are writing to the same the channel / key. \n",
    "\n",
    "As we touched on in Module 2, `operator.add` is a function from Python's built-in operator module.\n",
    "\n",
    "When `operator.add` is applied to lists, it performs list concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1292ac-510a-4801-b2a3-e2c6d2d9582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: Annotated[list, operator.add]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffbad231-fc1d-49b1-a9fc-ed9153fa3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding I'm A to []\n",
      "Adding I'm B to [\"I'm A\"]\n",
      "Adding I'm C to [\"I'm A\"]\n",
      "Adding I'm D to [\"I'm A\", \"I'm B\", \"I'm C\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm D\"]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5baa2-cecd-44b6-b0c4-d258340783f8",
   "metadata": {},
   "source": [
    "Now we see that we append to state for the updates made in parallel by `b` and `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fc7c7-198d-41be-867f-e77c93ba3217",
   "metadata": {},
   "source": [
    "## Waiting for nodes to finish\n",
    "\n",
    "Now, lets consider a case where one parallel path has more steps than the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f50b5d4f-dd39-4c22-b623-e0abc23f9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11640e6f-ac62-4ad4-89d9-7f6f9b56bf7a",
   "metadata": {},
   "source": [
    "In this case, `b`, `b2`, and `c` are all part of the same step.\n",
    "\n",
    "The graph will wait for all of these to be completed before proceeding to step `d`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fafda930-e75b-410f-ba93-eb5fc0219303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding I'm A to []\n",
      "Adding I'm B to [\"I'm A\"]\n",
      "Adding I'm C to [\"I'm A\"]\n",
      "Adding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\n",
      "Adding I'm D to [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\", \"I'm D\"]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610a2e3-b053-47e8-bf4e-0968dfaa0a5d",
   "metadata": {},
   "source": [
    "## Setting the order of state updates\n",
    "\n",
    "However, within each step we don't have specific control over the order of the state updates!\n",
    "\n",
    "In simple terms, it is a deterministic order determined by LangGraph based upon graph topology that **we do not control**. \n",
    "\n",
    "Above, we see that `c` is added before `b2`.\n",
    "\n",
    "However, we can use a custom reducer to customize this e.g., sort state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24788e73-0950-432e-ad32-7987ea076529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_reducer(left, right):\n",
    "    \"\"\" Combines and sorts the values in a list\"\"\"\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    \n",
    "    return sorted(left + right, reverse=False)\n",
    "\n",
    "class State(TypedDict):\n",
    "    # sorting_reducer will sort the values in state\n",
    "    state: Annotated[list, sorting_reducer]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "607dba2e-f9f0-4bc7-8ba6-684521a49bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding I'm A to []\n",
      "Adding I'm C to [\"I'm A\"]\n",
      "Adding I'm B to [\"I'm A\"]\n",
      "Adding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\n",
      "Adding I'm D to [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\", \"I'm D\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1714c0-e881-48e7-bcb8-a60016f0485e",
   "metadata": {},
   "source": [
    "Now, the reducer sorts the updated state values!\n",
    "\n",
    "The `sorting_reducer` example sorts all values globally. We can also: \n",
    "\n",
    "1. Write outputs to a separate field in the state during the parallel step\n",
    "2. Use a \"sink\" node after the parallel step to combine and order those outputs\n",
    "3. Clear the temporary field after combining\n",
    "\n",
    "See the [docs](https://langchain-ai.github.io/langgraph/how-tos/branching/#stable-sorting) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0750b-e6af-40d9-835c-c664da5a2d3b",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "\n",
    "Now, lets add a realistic example! \n",
    "\n",
    "We want to gather context from two external sources (Wikipedia and Web-Search) and have an LLM answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e9d03c-cb41-415c-862d-c9616d5a2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(model=\"gpt-4.1-mini\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f75cc78-d1a1-47a5-8648-bf5a79c883de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e714ea8-095c-461a-98bc-ee782a84ef5c",
   "metadata": {},
   "source": [
    "You can try different web search tools. [Tavily](https://tavily.com/) is one nice option to consider, but ensure your `TAVILY_API_KEY` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bb519a-d08a-4ec7-8f0b-2ce6a9bf7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb4f56c-3334-4927-8ed8-62fd384ee43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "def search_web(state):\n",
    "    \n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search\n",
    "    tavily_search = TavilySearchResults(max_results=3)\n",
    "    search_docs = tavily_search.invoke(state['question'])\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\">\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "def search_wikipedia(state):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=state['question'], \n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "def generate_answer(state):\n",
    "    \n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    context = state[\"context\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Template\n",
    "    answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"\n",
    "    answer_instructions = answer_template.format(question=question, \n",
    "                                                       context=context)    \n",
    "    \n",
    "    # Answer\n",
    "    answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])\n",
    "      \n",
    "    # Append it to state\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"search_web\",search_web)\n",
    "builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"search_wikipedia\")\n",
    "builder.add_edge(START, \"search_web\")\n",
    "builder.add_edge(\"search_wikipedia\", \"generate_answer\")\n",
    "builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa544ca0-10af-491e-ad7a-477d004413eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21040/443896071.py:11: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nvidia's Q2 2024 earnings were exceptionally strong. The company reported:\\n\\n- Operating income of $6.8 billion, up 218% from the previous quarter and up 1,263% year-over-year.\\n- Net income of $6.188 billion, up 203% from the previous quarter and up 843% year-over-year.\\n- GAAP diluted earnings per share of $2.48, which increased 202% from the previous quarter and surged 854% compared to a year ago.\\n- Non-GAAP diluted earnings per share of $2.70, up 148% from the previous quarter and up 429% year-over-year.\\n\\nThese results reflect a significant growth driven by the transition to accelerated computing and generative AI, as highlighted by Nvidia's CEO Jensen Huang. Overall, Nvidia's Q2 2024 earnings demonstrated remarkable profitability and growth.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"How were Nvidia's Q2 2024 earnings\"})\n",
    "result['answer'].content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dbbecab-80eb-4f0c-b43a-45542fc0ae9c",
   "metadata": {},
   "source": [
    "## Using with LangGraph API\n",
    "\n",
    "**âš ï¸ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- ðŸš€ API: http://127.0.0.1:2024\n",
    "- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ðŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc8ad8d-1365-4801-a8a5-b85cd4965119",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23919dc9-27d8-4d10-b91d-24acdf8c0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff35e68f-4017-4f45-93cf-ddbb355a0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '1f06b067-0eec-6710-80ea-e8f3adee5230', 'attempt': 1})\n",
      "StreamPart(event='values', data={'question': 'How were Nvidia Q2 2024 earnings?', 'context': []})\n",
      "StreamPart(event='', data=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(event)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Check if answer has been added to state  \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m answer = \u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(answer[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_question = {\"question\": \"How were Nvidia Q2 2024 earnings?\"}\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"parallelization\", \n",
    "                                      input=input_question, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    # Check if answer has been added to state  \n",
    "    answer = event.data.get('answer', None)\n",
    "    if answer:\n",
    "        print(answer['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
